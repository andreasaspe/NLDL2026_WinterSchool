Found 3241 subjects in 'train' split from CSV.
Found 3237 subjects with both ECG data and mask files in 'train' split.
Found 811 subjects in 'val' split from CSV.
Found 811 subjects with both ECG data and mask files in 'val' split.
/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:128: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(init_scale=2**10, growth_interval=1000)
Epoch: 1:   0%|                                                | 0/101 [00:00<?, ?it/s]/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:154: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch: 1: 100%|████████████| 101/101 [02:37<00:00,  1.56s/it, loss=3.4868, lr=5.00e-06]
  Epoch 1: train_loss=3.5993 | lr=1.00e-05 | logit_scale=2.66 (exp=14.3)
Validation after epoch: 1:   0%|                                | 0/26 [00:00<?, ?it/s]/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Validation after epoch: 1: 100%|███████████████████████| 26/26 [00:08<00:00,  3.05it/s]
  Epoch 1: val_loss=3.4279 | train-val gap=0.1714
  ✓ New best model saved (val_loss=3.4279)
Epoch: 2: 100%|████████████| 101/101 [02:31<00:00,  1.50s/it, loss=3.4612, lr=1.00e-05]
  Epoch 2: train_loss=3.5035 | lr=1.50e-05 | logit_scale=2.66 (exp=14.3)
Validation after epoch: 2: 100%|███████████████████████| 26/26 [00:08<00:00,  3.23it/s]
  Epoch 2: val_loss=3.4259 | train-val gap=0.0776
  ✓ New best model saved (val_loss=3.4259)
Epoch: 3: 100%|████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4723, lr=1.50e-05]
  Epoch 3: train_loss=3.4813 | lr=2.00e-05 | logit_scale=2.66 (exp=14.3)
Validation after epoch: 3: 100%|███████████████████████| 26/26 [00:07<00:00,  3.30it/s]
  Epoch 3: val_loss=3.4247 | train-val gap=0.0566
  ✓ New best model saved (val_loss=3.4247)
Epoch: 4: 100%|████████████| 101/101 [02:35<00:00,  1.54s/it, loss=3.4639, lr=2.00e-05]
  Epoch 4: train_loss=3.4744 | lr=2.50e-05 | logit_scale=2.66 (exp=14.2)
Validation after epoch: 4: 100%|███████████████████████| 26/26 [00:07<00:00,  3.28it/s]
  Epoch 4: val_loss=3.4243 | train-val gap=0.0502
  ✓ New best model saved (val_loss=3.4243)
Epoch: 5: 100%|████████████| 101/101 [02:35<00:00,  1.54s/it, loss=3.4840, lr=2.50e-05]
  Epoch 5: train_loss=3.4694 | lr=3.00e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 5: 100%|████████████████████████████████| 26/26 [00:07<00:00,  3.28it/s]
  Epoch 5: val_loss=3.4258 | train-val gap=0.0436
Epoch: 6: 100%|█████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4515, lr=3.00e-05]
  Epoch 6: train_loss=3.4668 | lr=3.50e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 6: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 6: val_loss=3.4229 | train-val gap=0.0439
  ✓ New best model saved (val_loss=3.4229)
Epoch: 7: 100%|█████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4474, lr=3.50e-05]
  Epoch 7: train_loss=3.4592 | lr=4.00e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 7: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.25it/s]
  Epoch 7: val_loss=3.4205 | train-val gap=0.0388
  ✓ New best model saved (val_loss=3.4205)
Epoch: 8: 100%|█████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4634, lr=4.00e-05]
  Epoch 8: train_loss=3.4627 | lr=4.50e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 8: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.21it/s]
  Epoch 8: val_loss=3.4231 | train-val gap=0.0396
Epoch: 9: 100%|█████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4393, lr=4.50e-05]
  Epoch 9: train_loss=3.4630 | lr=5.00e-05 | logit_scale=2.65 (exp=14.1)
Validation after epoch: 9: 100%|████████████████████████████████| 26/26 [00:07<00:00,  3.31it/s]
  Epoch 9: val_loss=3.4221 | train-val gap=0.0409
Epoch: 10: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4487, lr=5.00e-05]
  Epoch 10: train_loss=3.4619 | lr=5.00e-05 | logit_scale=2.65 (exp=14.1)
Validation after epoch: 10: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.35it/s]
  Epoch 10: val_loss=3.4206 | train-val gap=0.0413
Epoch: 11: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4624, lr=5.00e-05]
  Epoch 11: train_loss=3.4624 | lr=5.00e-05 | logit_scale=2.65 (exp=14.1)
Validation after epoch: 11: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.34it/s]
  Epoch 11: val_loss=3.4211 | train-val gap=0.0412
Epoch: 12: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4494, lr=5.00e-05]
  Epoch 12: train_loss=3.4623 | lr=5.00e-05 | logit_scale=2.65 (exp=14.1)
Validation after epoch: 12: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.31it/s]
  Epoch 12: val_loss=3.4115 | train-val gap=0.0507
  ✓ New best model saved (val_loss=3.4115)
Epoch: 13: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4845, lr=5.00e-05]
  Epoch 13: train_loss=3.4595 | lr=5.00e-05 | logit_scale=2.65 (exp=14.1)
Validation after epoch: 13: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.28it/s]
  Epoch 13: val_loss=3.4200 | train-val gap=0.0395
Epoch: 14: 100%|████████████████████| 101/101 [02:35<00:00,  1.54s/it, loss=3.4701, lr=5.00e-05]
  Epoch 14: train_loss=3.4606 | lr=5.00e-05 | logit_scale=2.64 (exp=14.1)
Validation after epoch: 14: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.31it/s]
  Epoch 14: val_loss=3.4233 | train-val gap=0.0374
Epoch: 15: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4648, lr=5.00e-05]
  Epoch 15: train_loss=3.4578 | lr=5.00e-05 | logit_scale=2.64 (exp=14.1)
Validation after epoch: 15: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.28it/s]
  Epoch 15: val_loss=3.4173 | train-val gap=0.0406
Epoch: 16: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4447, lr=5.00e-05]
  Epoch 16: train_loss=3.4578 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 16: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.32it/s]
  Epoch 16: val_loss=3.4224 | train-val gap=0.0354
Epoch: 17: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4395, lr=5.00e-05]
  Epoch 17: train_loss=3.4564 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 17: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.31it/s]
  Epoch 17: val_loss=3.4228 | train-val gap=0.0336
Epoch: 18: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4443, lr=5.00e-05]
  Epoch 18: train_loss=3.4552 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 18: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.31it/s]
  Epoch 18: val_loss=3.4129 | train-val gap=0.0423
Epoch: 19: 100%|████████████████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4504, lr=5.00e-05]
  Epoch 19: train_loss=3.4530 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 19: 100%|███████████████████████████████| 26/26 [00:07<00:00,  3.27it/s]
  Epoch 19: val_loss=3.4120 | train-val gap=0.0410
Epoch: 20: 100%|████████████████████| 101/101 [02:35<00:00,  1.54s/it, loss=3.4504, lr=5.00e-05]
  Epoch 20: train_loss=3.4570 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 20: 100%|██████████████████████| 26/26 [00:07<00:00,  3.25it/s]
  Epoch 20: val_loss=3.4177 | train-val gap=0.0393
Epoch: 21: 100%|███████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4533, lr=5.00e-05]
  Epoch 21: train_loss=3.4555 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 21: 100%|██████████████████████| 26/26 [00:07<00:00,  3.35it/s]
  Epoch 21: val_loss=3.4115 | train-val gap=0.0439
  ✓ New best model saved (val_loss=3.4115)
Epoch: 22: 100%|███████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4583, lr=5.00e-05]
  Epoch 22: train_loss=3.4506 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 22: 100%|██████████████████████| 26/26 [00:07<00:00,  3.34it/s]
  Epoch 22: val_loss=3.4127 | train-val gap=0.0379
Epoch: 23: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4291, lr=5.00e-05]
  Epoch 23: train_loss=3.4569 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 23: 100%|██████████████████████| 26/26 [00:07<00:00,  3.31it/s]
  Epoch 23: val_loss=3.4152 | train-val gap=0.0417
Epoch: 24: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4796, lr=5.00e-05]
  Epoch 24: train_loss=3.4569 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 24: 100%|██████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 24: val_loss=3.4133 | train-val gap=0.0436
Epoch: 25: 100%|███████████| 101/101 [02:38<00:00,  1.56s/it, loss=3.4921, lr=5.00e-05]
  Epoch 25: train_loss=3.4561 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 25: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 25: val_loss=3.4183 | train-val gap=0.0379
Epoch: 26: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4406, lr=5.00e-05]
  Epoch 26: train_loss=3.4582 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 26: 100%|██████████████████████| 26/26 [00:07<00:00,  3.26it/s]
  Epoch 26: val_loss=3.4143 | train-val gap=0.0439
Epoch: 27: 100%|███████████| 101/101 [02:34<00:00,  1.53s/it, loss=3.4313, lr=5.00e-05]
  Epoch 27: train_loss=3.4531 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 27: 100%|██████████████████████| 26/26 [00:07<00:00,  3.34it/s]
  Epoch 27: val_loss=3.4084 | train-val gap=0.0447
  ✓ New best model saved (val_loss=3.4084)
Epoch: 28: 100%|███████████| 101/101 [02:36<00:00,  1.55s/it, loss=3.4385, lr=5.00e-05]
  Epoch 28: train_loss=3.4500 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 28: 100%|██████████████████████| 26/26 [00:08<00:00,  3.11it/s]
  Epoch 28: val_loss=3.4083 | train-val gap=0.0416
  ✓ New best model saved (val_loss=3.4083)
Epoch: 29: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.4701, lr=5.00e-05]
  Epoch 29: train_loss=3.4531 | lr=5.00e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 29: 100%|██████████████████████| 26/26 [00:08<00:00,  3.24it/s]
  Epoch 29: val_loss=3.4091 | train-val gap=0.0441
Epoch: 30: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.4132, lr=5.00e-05]
  Epoch 30: train_loss=3.4482 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 30: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 30: val_loss=3.4018 | train-val gap=0.0465
  ✓ New best model saved (val_loss=3.4018)
Epoch: 31: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.4468, lr=4.99e-05]
  Epoch 31: train_loss=3.4467 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 31: 100%|██████████████████████| 26/26 [00:08<00:00,  3.13it/s]
  Epoch 31: val_loss=3.4048 | train-val gap=0.0419
Epoch: 32: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.5164, lr=4.99e-05]
  Epoch 32: train_loss=3.4459 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 32: 100%|██████████████████████| 26/26 [00:08<00:00,  3.11it/s]
  Epoch 32: val_loss=3.4050 | train-val gap=0.0409
Epoch: 33: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.4158, lr=4.99e-05]
  Epoch 33: train_loss=3.4420 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 33: 100%|██████████████████████| 26/26 [00:08<00:00,  3.10it/s]
  Epoch 33: val_loss=3.4190 | train-val gap=0.0230
Epoch: 34: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3967, lr=4.99e-05]
  Epoch 34: train_loss=3.4417 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 34: 100%|██████████████████████| 26/26 [00:08<00:00,  3.11it/s]
  Epoch 34: val_loss=3.4071 | train-val gap=0.0346
Epoch: 35: 100%|███████████| 101/101 [02:42<00:00,  1.61s/it, loss=3.4341, lr=4.99e-05]
  Epoch 35: train_loss=3.4424 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 35: 100%|██████████████████████| 26/26 [00:08<00:00,  3.06it/s]
  Epoch 35: val_loss=3.4022 | train-val gap=0.0402
Epoch: 36: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4823, lr=4.99e-05]
  Epoch 36: train_loss=3.4380 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 36: 100%|██████████████████████| 26/26 [00:08<00:00,  3.09it/s]
  Epoch 36: val_loss=3.4028 | train-val gap=0.0352
Epoch: 37: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4476, lr=4.99e-05]
  Epoch 37: train_loss=3.4343 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 37: 100%|██████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 37: val_loss=3.4102 | train-val gap=0.0241
Epoch: 38: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.4616, lr=4.99e-05]
  Epoch 38: train_loss=3.4354 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 38: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 38: val_loss=3.4078 | train-val gap=0.0276
Epoch: 39: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.3928, lr=4.99e-05]
  Epoch 39: train_loss=3.4312 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 39: 100%|██████████████████████| 26/26 [00:08<00:00,  3.21it/s]
  Epoch 39: val_loss=3.4087 | train-val gap=0.0225
Epoch: 40: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4053, lr=4.99e-05]
  Epoch 40: train_loss=3.4357 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 40: 100%|██████████████████████| 26/26 [00:08<00:00,  3.12it/s]
  Epoch 40: val_loss=3.4146 | train-val gap=0.0211
Epoch: 41: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.3705, lr=4.99e-05]
  Epoch 41: train_loss=3.4272 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 41: 100%|██████████████████████| 26/26 [00:08<00:00,  3.10it/s]
  Epoch 41: val_loss=3.4346 | train-val gap=-0.0074
Epoch: 42: 100%|███████████| 101/101 [02:43<00:00,  1.62s/it, loss=3.4750, lr=4.99e-05]
  Epoch 42: train_loss=3.4283 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 42: 100%|██████████████████████| 26/26 [00:07<00:00,  3.27it/s]
  Epoch 42: val_loss=3.4222 | train-val gap=0.0061
Epoch: 43: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.4618, lr=4.99e-05]
  Epoch 43: train_loss=3.4290 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 43: 100%|██████████████████████| 26/26 [00:07<00:00,  3.26it/s]
  Epoch 43: val_loss=3.4092 | train-val gap=0.0198
Epoch: 44: 100%|███████████| 101/101 [02:37<00:00,  1.56s/it, loss=3.4020, lr=4.99e-05]
  Epoch 44: train_loss=3.4249 | lr=4.99e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 44: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 44: val_loss=3.4099 | train-val gap=0.0149
Epoch: 45: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.3848, lr=4.99e-05]
  Epoch 45: train_loss=3.4216 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 45: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 45: val_loss=3.4135 | train-val gap=0.0082
Epoch: 46: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4413, lr=4.98e-05]
  Epoch 46: train_loss=3.4269 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 46: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 46: val_loss=3.4094 | train-val gap=0.0176
Epoch: 47: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.5307, lr=4.98e-05]
  Epoch 47: train_loss=3.4233 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 47: 100%|██████████████████████| 26/26 [00:08<00:00,  3.21it/s]
  Epoch 47: val_loss=3.4115 | train-val gap=0.0119
Epoch: 48: 100%|███████████| 101/101 [02:40<00:00,  1.58s/it, loss=3.4096, lr=4.98e-05]
  Epoch 48: train_loss=3.4228 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 48: 100%|██████████████████████| 26/26 [00:08<00:00,  3.24it/s]
  Epoch 48: val_loss=3.4063 | train-val gap=0.0165
Epoch: 49: 100%|███████████| 101/101 [02:43<00:00,  1.61s/it, loss=3.4402, lr=4.98e-05]
  Epoch 49: train_loss=3.4207 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 49: 100%|██████████████████████| 26/26 [00:08<00:00,  3.23it/s]
  Epoch 49: val_loss=3.4083 | train-val gap=0.0124
Epoch: 50: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.3789, lr=4.98e-05]
  Epoch 50: train_loss=3.4210 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 50: 100%|██████████████████████| 26/26 [00:08<00:00,  3.21it/s]
  Epoch 50: val_loss=3.4150 | train-val gap=0.0059
Epoch: 51: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4665, lr=4.98e-05]
  Epoch 51: train_loss=3.4194 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 51: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 51: val_loss=3.4166 | train-val gap=0.0027
Epoch: 52: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.4207, lr=4.98e-05]
  Epoch 52: train_loss=3.4121 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 52: 100%|██████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 52: val_loss=3.4191 | train-val gap=-0.0070
Epoch: 53: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3406, lr=4.98e-05]
  Epoch 53: train_loss=3.4146 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 53: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 53: val_loss=3.4063 | train-val gap=0.0083
Epoch: 54: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.4422, lr=4.98e-05]
  Epoch 54: train_loss=3.4095 | lr=4.98e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 54: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 54: val_loss=3.3973 | train-val gap=0.0122
  ✓ New best model saved (val_loss=3.3973)
Epoch: 55: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4075, lr=4.98e-05]
  Epoch 55: train_loss=3.4099 | lr=4.97e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 55: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 55: val_loss=3.4080 | train-val gap=0.0019
Epoch: 56: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3278, lr=4.97e-05]
  Epoch 56: train_loss=3.4103 | lr=4.97e-05 | logit_scale=2.64 (exp=13.9)
Validation after epoch: 56: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 56: val_loss=3.4109 | train-val gap=-0.0006
Epoch: 57: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.2989, lr=4.97e-05]
  Epoch 57: train_loss=3.4091 | lr=4.97e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 57: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 57: val_loss=3.4181 | train-val gap=-0.0089
Epoch: 58: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3271, lr=4.97e-05]
  Epoch 58: train_loss=3.4067 | lr=4.97e-05 | logit_scale=2.64 (exp=14.0)
Validation after epoch: 58: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 58: val_loss=3.4233 | train-val gap=-0.0167
Epoch: 59: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3858, lr=4.97e-05]
  Epoch 59: train_loss=3.4046 | lr=4.97e-05 | logit_scale=2.64 (exp=13.9)
Validation after epoch: 59: 100%|██████████████████████| 26/26 [00:08<00:00,  3.22it/s]
  Epoch 59: val_loss=3.4176 | train-val gap=-0.0130
Epoch: 60: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3965, lr=4.97e-05]
  ⚠ Inf/NaN gradients at epoch 60, step 91 — skipping step
  Epoch 60: train_loss=3.3968 | lr=4.97e-05 | logit_scale=2.64 (exp=13.9)
Validation after epoch: 60: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 60: val_loss=3.4231 | train-val gap=-0.0263
Epoch: 61: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4595, lr=4.97e-05]
  Epoch 61: train_loss=3.3981 | lr=4.97e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 61: 100%|██████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 61: val_loss=3.4274 | train-val gap=-0.0293
Epoch: 62: 100%|███████████| 101/101 [02:39<00:00,  1.57s/it, loss=3.3724, lr=4.97e-05]
  Epoch 62: train_loss=3.4017 | lr=4.97e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 62: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 62: val_loss=3.4197 | train-val gap=-0.0180
Epoch: 63: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.3123, lr=4.97e-05]
  Epoch 63: train_loss=3.4077 | lr=4.96e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 63: 100%|██████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 63: val_loss=3.4143 | train-val gap=-0.0066
Epoch: 64: 100%|███████████| 101/101 [02:39<00:00,  1.57s/it, loss=3.3428, lr=4.96e-05]
  Epoch 64: train_loss=3.3910 | lr=4.96e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 64: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 64: val_loss=3.3993 | train-val gap=-0.0083
Epoch: 65: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3966, lr=4.96e-05]
  Epoch 65: train_loss=3.4029 | lr=4.96e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 65: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 65: val_loss=3.4136 | train-val gap=-0.0106
Epoch: 66: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3145, lr=4.96e-05]
  Epoch 66: train_loss=3.3946 | lr=4.96e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 66: 100%|██████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 66: val_loss=3.4240 | train-val gap=-0.0294
Epoch: 67: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3320, lr=4.96e-05]
  Epoch 67: train_loss=3.3949 | lr=4.96e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 67: 100%|██████████████████████| 26/26 [00:08<00:00,  3.14it/s]
  Epoch 67: val_loss=3.4061 | train-val gap=-0.0112
Epoch: 68: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4044, lr=4.96e-05]
  Epoch 68: train_loss=3.3873 | lr=4.96e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 68: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 68: val_loss=3.4336 | train-val gap=-0.0463
Epoch: 69: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4167, lr=4.96e-05]
  ⚠ Inf/NaN gradients at epoch 69, step 75 — skipping step
  Epoch 69: train_loss=3.3981 | lr=4.96e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 69: 100%|██████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 69: val_loss=3.4270 | train-val gap=-0.0289
Epoch: 70: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3746, lr=4.96e-05]
  Epoch 70: train_loss=3.3888 | lr=4.95e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 70: 100%|██████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 70: val_loss=3.4186 | train-val gap=-0.0298
Epoch: 71: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.3790, lr=4.95e-05]
  Epoch 71: train_loss=3.3973 | lr=4.95e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 71: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 71: val_loss=3.4167 | train-val gap=-0.0194
Epoch: 72: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.4232, lr=4.95e-05]
  Epoch 72: train_loss=3.3922 | lr=4.95e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 72: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 72: val_loss=3.4151 | train-val gap=-0.0229
Epoch: 73: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.5261, lr=4.95e-05]
  Epoch 73: train_loss=3.3763 | lr=4.95e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 73: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 73: val_loss=3.4723 | train-val gap=-0.0960
Epoch: 74: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3905, lr=4.95e-05]
  Epoch 74: train_loss=3.3801 | lr=4.95e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 74: 100%|██████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 74: val_loss=3.4832 | train-val gap=-0.1031
Epoch: 75: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3737, lr=4.95e-05]
  Epoch 75: train_loss=3.3896 | lr=4.95e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 75: 100%|██████████████████████| 26/26 [00:07<00:00,  3.27it/s]
  Epoch 75: val_loss=3.4218 | train-val gap=-0.0322
Epoch: 76: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.4197, lr=4.95e-05]
  Epoch 76: train_loss=3.3823 | lr=4.95e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 76: 100%|██████████████████████| 26/26 [00:08<00:00,  3.24it/s]
  Epoch 76: val_loss=3.4496 | train-val gap=-0.0672
Epoch: 77: 100%|███████████| 101/101 [02:42<00:00,  1.61s/it, loss=3.4268, lr=4.95e-05]
  Epoch 77: train_loss=3.3775 | lr=4.94e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 77: 100%|██████████████████████| 26/26 [00:08<00:00,  3.23it/s]
  Epoch 77: val_loss=3.4223 | train-val gap=-0.0448
Epoch: 78: 100%|███████████| 101/101 [02:42<00:00,  1.61s/it, loss=3.4188, lr=4.94e-05]
  Epoch 78: train_loss=3.3794 | lr=4.94e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 78: 100%|██████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 78: val_loss=3.4818 | train-val gap=-0.1024
Epoch: 79: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3066, lr=4.94e-05]
  Epoch 79: train_loss=3.3781 | lr=4.94e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 79: 100%|██████████████████████| 26/26 [00:08<00:00,  3.22it/s]
  Epoch 79: val_loss=3.5006 | train-val gap=-0.1225
Epoch: 80: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3213, lr=4.94e-05]
  Epoch 80: train_loss=3.3672 | lr=4.94e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 80: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 80: val_loss=3.4708 | train-val gap=-0.1036
Epoch: 81: 100%|███████████| 101/101 [02:42<00:00,  1.61s/it, loss=3.4280, lr=4.94e-05]
  Epoch 81: train_loss=3.3732 | lr=4.94e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 81: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 81: val_loss=3.4574 | train-val gap=-0.0842
Epoch: 82: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4079, lr=4.94e-05]
  Epoch 82: train_loss=3.3702 | lr=4.94e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 82: 100%|██████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 82: val_loss=3.4296 | train-val gap=-0.0594
Epoch: 83: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.2589, lr=4.94e-05]
  Epoch 83: train_loss=3.3686 | lr=4.93e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 83: 100%|██████████████████████| 26/26 [00:08<00:00,  3.22it/s]
  Epoch 83: val_loss=3.4700 | train-val gap=-0.1013
Epoch: 84: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3610, lr=4.93e-05]
  Epoch 84: train_loss=3.3610 | lr=4.93e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 84: 100%|██████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 84: val_loss=3.4779 | train-val gap=-0.1169
Epoch: 85: 100%|███████████| 101/101 [02:42<00:00,  1.61s/it, loss=3.2758, lr=4.93e-05]
  Epoch 85: train_loss=3.3592 | lr=4.93e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 85: 100%|██████████████████████| 26/26 [00:08<00:00,  3.23it/s]
  Epoch 85: val_loss=3.4845 | train-val gap=-0.1252
Epoch: 86: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3605, lr=4.93e-05]
  ⚠ Inf/NaN gradients at epoch 86, step 53 — skipping step
  Epoch 86: train_loss=3.3609 | lr=4.93e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 86: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 86: val_loss=3.4526 | train-val gap=-0.0917
Epoch: 87: 100%|███████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3230, lr=4.93e-05]
  Epoch 87: train_loss=3.3573 | lr=4.93e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 87: 100%|██████████████████████| 26/26 [00:08<00:00,  3.21it/s]
  Epoch 87: val_loss=3.4864 | train-val gap=-0.1291
Epoch: 88: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3028, lr=4.93e-05]
  Epoch 88: train_loss=3.3564 | lr=4.92e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 88: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 88: val_loss=3.4792 | train-val gap=-0.1228
Epoch: 89: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3263, lr=4.92e-05]
  Epoch 89: train_loss=3.3574 | lr=4.92e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 89: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 89: val_loss=3.5080 | train-val gap=-0.1507
Epoch: 90: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.2481, lr=4.92e-05]
  Epoch 90: train_loss=3.3548 | lr=4.92e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 90: 100%|██████████████████████| 26/26 [00:08<00:00,  3.14it/s]
  Epoch 90: val_loss=3.4924 | train-val gap=-0.1375
Epoch: 91: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3425, lr=4.92e-05]
  Epoch 91: train_loss=3.3415 | lr=4.92e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 91: 100%|██████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 91: val_loss=3.4788 | train-val gap=-0.1373
Epoch: 92: 100%|███████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3771, lr=4.92e-05]
  Epoch 92: train_loss=3.3602 | lr=4.92e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 92: 100%|██████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 92: val_loss=3.4481 | train-val gap=-0.0879
Epoch: 93: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3525, lr=4.92e-05]
  Epoch 93: train_loss=3.3474 | lr=4.91e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 93: 100%|██████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 93: val_loss=3.4519 | train-val gap=-0.1045
Epoch: 94: 100%|███████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.2793, lr=4.91e-05]
  Epoch 94: train_loss=3.3491 | lr=4.91e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 94: 100%|██████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 94: val_loss=3.4752 | train-val gap=-0.1261
Epoch: 95: 100%|███████████| 101/101 [02:44<00:00,  1.63s/it, loss=3.4071, lr=4.91e-05]
  Epoch 95: train_loss=3.3409 | lr=4.91e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 95: 100%|██████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 95: val_loss=3.4886 | train-val gap=-0.1477
Epoch: 96: 100%|███████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4153, lr=4.91e-05]
  Epoch 96: train_loss=3.3405 | lr=4.91e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 96: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 96: val_loss=3.4968 | train-val gap=-0.1563
Epoch: 97: 100%|███████████| 101/101 [02:42<00:00,  1.61s/it, loss=3.4113, lr=4.91e-05]
  ⚠ Inf/NaN gradients at epoch 97, step 20 — skipping step
  Epoch 97: train_loss=3.3451 | lr=4.91e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 97: 100%|██████████████████████| 26/26 [00:08<00:00,  3.19it/s]
  Epoch 97: val_loss=3.4890 | train-val gap=-0.1439
Epoch: 98: 100%|███████████| 101/101 [02:39<00:00,  1.57s/it, loss=3.3573, lr=4.91e-05]
  Epoch 98: train_loss=3.3451 | lr=4.90e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 98: 100%|██████████████████████| 26/26 [00:08<00:00,  3.22it/s]
  Epoch 98: val_loss=3.4812 | train-val gap=-0.1361
Epoch: 99: 100%|███████████| 101/101 [02:41<00:00,  1.59s/it, loss=3.2377, lr=4.90e-05]
  Epoch 99: train_loss=3.3349 | lr=4.90e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 99: 100%|██████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 99: val_loss=3.5328 | train-val gap=-0.1979
Epoch: 100: 100%|██████████| 101/101 [02:45<00:00,  1.63s/it, loss=3.3975, lr=4.90e-05]
  Epoch 100: train_loss=3.3296 | lr=4.90e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 100: 100%|█████████████████████| 26/26 [00:08<00:00,  3.13it/s]
  Epoch 100: val_loss=3.5578 | train-val gap=-0.2282
Epoch: 101: 100%|██████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3471, lr=4.90e-05]
  Epoch 101: train_loss=3.3317 | lr=4.90e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 101: 100%|█████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 101: val_loss=3.4956 | train-val gap=-0.1639
Epoch: 102: 100%|██████████| 101/101 [02:43<00:00,  1.62s/it, loss=3.4202, lr=4.90e-05]
  Epoch 102: train_loss=3.3309 | lr=4.89e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 102: 100%|█████████████████████| 26/26 [00:08<00:00,  3.14it/s]
  Epoch 102: val_loss=3.5187 | train-val gap=-0.1877
Epoch: 103: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3090, lr=4.89e-05]
  Epoch 103: train_loss=3.3280 | lr=4.89e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 103: 100%|█████████████████████| 26/26 [00:08<00:00,  3.14it/s]
  Epoch 103: val_loss=3.5605 | train-val gap=-0.2325
Epoch: 104: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3233, lr=4.89e-05]
  Epoch 104: train_loss=3.3298 | lr=4.89e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 104: 100%|█████████████████████| 26/26 [00:08<00:00,  3.11it/s]
  Epoch 104: val_loss=3.5303 | train-val gap=-0.2005
Epoch: 105: 100%|██████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3385, lr=4.89e-05]
  Epoch 105: train_loss=3.3178 | lr=4.89e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 105: 100%|█████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 105: val_loss=3.5417 | train-val gap=-0.2240
Epoch: 106: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3233, lr=4.89e-05]
  Epoch 106: train_loss=3.3271 | lr=4.88e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 106: 100%|█████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 106: val_loss=3.5073 | train-val gap=-0.1801
Epoch: 107: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.2851, lr=4.88e-05]
  Epoch 107: train_loss=3.3071 | lr=4.88e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 107: 100%|█████████████████████| 26/26 [00:08<00:00,  3.14it/s]
  Epoch 107: val_loss=3.5477 | train-val gap=-0.2406
Epoch: 108: 100%|██████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3810, lr=4.88e-05]
  Epoch 108: train_loss=3.3273 | lr=4.88e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 108: 100%|█████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 108: val_loss=3.4872 | train-val gap=-0.1599
Epoch: 109: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3942, lr=4.88e-05]
  ⚠ Inf/NaN gradients at epoch 109, step 83 — skipping step
  Epoch 109: train_loss=3.3056 | lr=4.88e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 109: 100%|█████████████████████| 26/26 [00:08<00:00,  3.01it/s]
  Epoch 109: val_loss=3.5679 | train-val gap=-0.2623
Epoch: 110: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.2808, lr=4.88e-05]
  Epoch 110: train_loss=3.2996 | lr=4.88e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 110: 100%|█████████████████████| 26/26 [00:08<00:00,  3.03it/s]
  Epoch 110: val_loss=3.5340 | train-val gap=-0.2344
Epoch: 111: 100%|██████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.2871, lr=4.88e-05]
  Epoch 111: train_loss=3.3034 | lr=4.87e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 111: 100%|█████████████████████| 26/26 [00:08<00:00,  3.22it/s]
  Epoch 111: val_loss=3.5819 | train-val gap=-0.2785
Epoch: 112: 100%|██████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.3661, lr=4.87e-05]
  Epoch 112: train_loss=3.3107 | lr=4.87e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 112: 100%|█████████████████████| 26/26 [00:08<00:00,  3.23it/s]
  Epoch 112: val_loss=3.5442 | train-val gap=-0.2335
Epoch: 113: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3415, lr=4.87e-05]
  Epoch 113: train_loss=3.3018 | lr=4.87e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 113: 100%|█████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 113: val_loss=3.6018 | train-val gap=-0.3000
Epoch: 114: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3199, lr=4.87e-05]
  Epoch 114: train_loss=3.3052 | lr=4.87e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 114: 100%|█████████████████████| 26/26 [00:08<00:00,  3.24it/s]
  Epoch 114: val_loss=3.5262 | train-val gap=-0.2210
Epoch: 115: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3104, lr=4.87e-05]
  Epoch 115: train_loss=3.2947 | lr=4.86e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 115: 100%|█████████████████████| 26/26 [00:08<00:00,  3.23it/s]
  Epoch 115: val_loss=3.5712 | train-val gap=-0.2765
Epoch: 116: 100%|██████████| 101/101 [02:42<00:00,  1.60s/it, loss=3.3343, lr=4.86e-05]
  Epoch 116: train_loss=3.2929 | lr=4.86e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 116: 100%|█████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 116: val_loss=3.6155 | train-val gap=-0.3227
Epoch: 117: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3367, lr=4.86e-05]
  Epoch 117: train_loss=3.2935 | lr=4.86e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 117: 100%|█████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 117: val_loss=3.4999 | train-val gap=-0.2063
Epoch: 118: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3460, lr=4.86e-05]
  Epoch 118: train_loss=3.2756 | lr=4.85e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 118: 100%|█████████████████████| 26/26 [00:08<00:00,  3.13it/s]
  Epoch 118: val_loss=3.5912 | train-val gap=-0.3156
Epoch: 119: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.2572, lr=4.85e-05]
  ⚠ Inf/NaN gradients at epoch 119, step 97 — skipping step
  Epoch 119: train_loss=3.2777 | lr=4.85e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 119: 100%|█████████████████████| 26/26 [00:08<00:00,  3.08it/s]
  Epoch 119: val_loss=3.5565 | train-val gap=-0.2788
Epoch: 120: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.2697, lr=4.85e-05]
  Epoch 120: train_loss=3.2817 | lr=4.85e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 120: 100%|█████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 120: val_loss=3.5433 | train-val gap=-0.2616
Epoch: 121: 100%|██████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.2017, lr=4.85e-05]
  Epoch 121: train_loss=3.2766 | lr=4.85e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 121: 100%|█████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 121: val_loss=3.5201 | train-val gap=-0.2436
Epoch: 122: 100%|██████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.3322, lr=4.85e-05]
  Epoch 122: train_loss=3.2701 | lr=4.84e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 122: 100%|█████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 122: val_loss=3.6054 | train-val gap=-0.3353
Epoch: 123: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.3237, lr=4.84e-05]
  Epoch 123: train_loss=3.2729 | lr=4.84e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 123: 100%|█████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 123: val_loss=3.5250 | train-val gap=-0.2521
Epoch: 124: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.2472, lr=4.84e-05]
  Epoch 124: train_loss=3.2655 | lr=4.84e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 124: 100%|█████████████████████| 26/26 [00:08<00:00,  3.21it/s]
  Epoch 124: val_loss=3.5573 | train-val gap=-0.2918
Epoch: 125: 100%|██████████| 101/101 [02:42<00:00,  1.61s/it, loss=3.2898, lr=4.84e-05]
  Epoch 125: train_loss=3.2579 | lr=4.84e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 125: 100%|█████████████████████| 26/26 [00:08<00:00,  3.20it/s]
  Epoch 125: val_loss=3.5516 | train-val gap=-0.2937
Epoch: 126: 100%|██████████| 101/101 [02:43<00:00,  1.62s/it, loss=3.3719, lr=4.84e-05]
  Epoch 126: train_loss=3.2492 | lr=4.83e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 126: 100%|█████████████████████| 26/26 [00:08<00:00,  3.17it/s]
  Epoch 126: val_loss=3.5616 | train-val gap=-0.3124
Epoch: 127: 100%|██████████| 101/101 [02:41<00:00,  1.60s/it, loss=3.2510, lr=4.83e-05]
  Epoch 127: train_loss=3.2471 | lr=4.83e-05 | logit_scale=2.63 (exp=13.9)
Validation after epoch: 127: 100%|█████████████████████| 26/26 [00:08<00:00,  3.16it/s]
  Epoch 127: val_loss=3.5944 | train-val gap=-0.3472
Epoch: 128:  15%|█▋         | 15/101 [00:24<02:19,  1.63s/it, loss=3.2066, lr=4.83e-05]
  ⚠ NaN in logits at epoch 128, step 14
    logit_scale = 2.629 (exp=13.9)
Traceback (most recent call last):
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py", line 276, in <module>
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py", line 155, in train
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/model.py", line 324, in forward
    image_features = self.encode_image(image)
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/model.py", line 288, in encode_image
    return self.visual(image.type(self.dtype))
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/model.py", line 89, in forward
    x = self.layer2(x)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/container.py", line 253, in forward
    input = module(input)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/model.py", line 40, in forward
    out = self.relu2(self.bn2(self.conv2(out)))
                     ~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/modules/batchnorm.py", line 194, in forward
    return F.batch_norm(
           ~~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<11 lines>...
        self.eps,
        ^^^^^^^^^
    )
    ^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/nn/functional.py", line 2846, in batch_norm
    return torch.batch_norm(
           ~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
    ...<7 lines>...
        torch.backends.cudnn.enabled,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 864.00 MiB. GPU 1 has a total capacity of 94.97 GiB of which 386.81 MiB is free. Including non-PyTorch memory, this process has 94.51 GiB memory in use. Of the allocated memory 93.20 GiB is allocated by PyTorch, and 641.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
