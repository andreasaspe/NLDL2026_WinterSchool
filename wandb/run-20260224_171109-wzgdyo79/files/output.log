Found 3241 subjects in 'train' split from CSV.
Found 3237 subjects with both ECG data and mask files in 'train' split.
Found 811 subjects in 'val' split from CSV.
Found 811 subjects with both ECG data and mask files in 'val' split.
/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:109: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
Epoch: 1:   0%|                                                                                                                                                  | 0/102 [00:00<?, ?it/s]/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:129: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch: 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [02:35<00:00,  1.52s/it]
Validation after epoch: 1:   0%|                                                                                                                                  | 0/26 [00:00<?, ?it/s]/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:176: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Validation after epoch: 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:08<00:00,  3.17it/s]
Epoch: 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [02:32<00:00,  1.50s/it]
Validation after epoch: 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:08<00:00,  3.14it/s]
Epoch: 3:   5%|██████▊                                                                                                                                   | 5/102 [00:07<02:30,  1.55s/it]
Traceback (most recent call last):
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py", line 199, in <module>
    train()
    ~~~~~^^
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py", line 123, in train
    for step, batch in progress_bar:
                       ^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
               ^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/utils/data/dataloader.py", line 741, in __next__
    data = self._next_data()
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/utils/data/dataloader.py", line 801, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/clip_dataloader.py", line 284, in __getitem__
    return super().__getitem__(idx)
           ~~~~~~~~~~~~~~~~~~~^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/data/dataset.py", line 97, in __getitem__
    subject = self._transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/transform.py", line 158, in __call__
    transformed = self.apply_transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/composition.py", line 59, in apply_transform
    subject = transform(subject)  # type: ignore[assignment]
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/transform.py", line 158, in __call__
    transformed = self.apply_transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/spatial/random_flip.py", line 60, in apply_transform
    transformed = transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/transform.py", line 158, in __call__
    transformed = self.apply_transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/spatial/random_flip.py", line 92, in apply_transform
    _flip_image(image, axes)
    ~~~~~~~~~~~^^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/spatial/random_flip.py", line 128, in _flip_image
    data = np.ascontiguousarray(data)  # remove negative strides
KeyboardInterrupt
