Found 3241 subjects in 'train' split from CSV.
Found 3237 subjects with both ECG data and mask files in 'train' split.
Found 811 subjects in 'val' split from CSV.
Found 811 subjects with both ECG data and mask files in 'val' split.
/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:128: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(init_scale=2**10, growth_interval=1000)
Epoch: 1:   0%|                                                | 0/101 [00:00<?, ?it/s]/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:154: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Epoch: 1: 100%|████████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.5430, lr=5.00e-06]
  Epoch 1: train_loss=3.5948 | lr=1.00e-05 | logit_scale=2.66 (exp=14.3)
Validation after epoch: 1:   0%|                                | 0/26 [00:00<?, ?it/s]/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
Validation after epoch: 1: 100%|███████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 1: val_loss=3.4253 | train-val gap=0.1695
  ✓ New best model saved (val_loss=3.4253)
Epoch: 2: 100%|████████████| 101/101 [02:37<00:00,  1.56s/it, loss=3.4779, lr=1.00e-05]
  Epoch 2: train_loss=3.4957 | lr=1.50e-05 | logit_scale=2.66 (exp=14.3)
Validation after epoch: 2: 100%|███████████████████████| 26/26 [00:08<00:00,  3.11it/s]
  Epoch 2: val_loss=3.4232 | train-val gap=0.0725
  ✓ New best model saved (val_loss=3.4232)
Epoch: 3: 100%|████████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4865, lr=1.50e-05]
  Epoch 3: train_loss=3.4771 | lr=2.00e-05 | logit_scale=2.66 (exp=14.3)
Validation after epoch: 3: 100%|███████████████████████| 26/26 [00:08<00:00,  3.18it/s]
  Epoch 3: val_loss=3.4233 | train-val gap=0.0538
Epoch: 4: 100%|████████████| 101/101 [02:39<00:00,  1.58s/it, loss=3.4686, lr=2.00e-05]
  Epoch 4: train_loss=3.4697 | lr=2.50e-05 | logit_scale=2.66 (exp=14.2)
Validation after epoch: 4: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.11it/s]
  Epoch 4: val_loss=3.4238 | train-val gap=0.0459
Epoch: 5: 100%|█████████████████████| 101/101 [02:40<00:00,  1.59s/it, loss=3.4636, lr=2.50e-05]
  Epoch 5: train_loss=3.4668 | lr=3.00e-05 | logit_scale=2.66 (exp=14.2)
Validation after epoch: 5: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.15it/s]
  Epoch 5: val_loss=3.4225 | train-val gap=0.0443
  ✓ New best model saved (val_loss=3.4225)
Epoch: 6: 100%|█████████████████████| 101/101 [02:37<00:00,  1.56s/it, loss=3.4176, lr=3.00e-05]
  Epoch 6: train_loss=3.4640 | lr=3.50e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 6: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.08it/s]
  Epoch 6: val_loss=3.4205 | train-val gap=0.0435
  ✓ New best model saved (val_loss=3.4205)
Epoch: 7: 100%|█████████████████████| 101/101 [02:36<00:00,  1.55s/it, loss=3.4469, lr=3.50e-05]
  Epoch 7: train_loss=3.4647 | lr=4.00e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 7: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.11it/s]
  Epoch 7: val_loss=3.4191 | train-val gap=0.0456
  ✓ New best model saved (val_loss=3.4191)
Epoch: 8: 100%|█████████████████████| 101/101 [02:37<00:00,  1.56s/it, loss=3.4617, lr=4.00e-05]
  Epoch 8: train_loss=3.4604 | lr=4.50e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 8: 100%|████████████████████████████████| 26/26 [00:08<00:00,  3.10it/s]
  Epoch 8: val_loss=3.4235 | train-val gap=0.0370
Epoch: 9: 100%|█████████████████████| 101/101 [02:38<00:00,  1.57s/it, loss=3.4613, lr=4.50e-05]
  Epoch 9: train_loss=3.4625 | lr=5.00e-05 | logit_scale=2.65 (exp=14.2)
Validation after epoch: 9: 100%|████████████████████████████████| 26/26 [00:07<00:00,  3.31it/s]
  Epoch 9: val_loss=3.4216 | train-val gap=0.0408
Epoch: 10:  23%|████▊                | 23/101 [00:36<02:03,  1.58s/it, loss=3.4654, lr=5.00e-05]
Traceback (most recent call last):
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py", line 276, in <module>
    train()
    ~~~~~^^
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/train_clip3d_ecg.py", line 143, in train
    for step, batch in progress_bar:
                       ^^^^^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
               ^^^^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/utils/data/dataloader.py", line 741, in __next__
    data = self._next_data()
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/utils/data/dataloader.py", line 801, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "/home/awias/Documents/code/NLDL2026_WinterSchool/3DCLIP/clip_dataloader.py", line 284, in __getitem__
    return super().__getitem__(idx)
           ~~~~~~~~~~~~~~~~~~~^^^^^
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/data/dataset.py", line 97, in __getitem__
    subject = self._transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/transform.py", line 158, in __call__
    transformed = self.apply_transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/composition.py", line 59, in apply_transform
    subject = transform(subject)  # type: ignore[assignment]
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/transform.py", line 158, in __call__
    transformed = self.apply_transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/spatial/random_affine.py", line 199, in apply_transform
    transformed = transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/transform.py", line 158, in __call__
    transformed = self.apply_transform(subject)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/spatial/random_affine.py", line 422, in apply_transform
    transformed_tensor = self.apply_affine_transform(
        sitk_image,
    ...<2 lines>...
        default_value,
    )
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/torchio/transforms/augmentation/spatial/random_affine.py", line 447, in apply_affine_transform
    resampled = resampler.Execute(floating)
  File "/home/awias/miniconda3/envs/standard/lib/python3.14/site-packages/SimpleITK/SimpleITK.py", line 50364, in Execute
    return _SimpleITK.ResampleImageFilter_Execute(self, image1)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
KeyboardInterrupt
